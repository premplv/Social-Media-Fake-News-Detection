{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhavani-Rupa/Social-Media-Fake-News-Detection/blob/main/Social_Media_Fake_News_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StjCq0d2AlL6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPtJ-lLOscos"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "true_path = '/content/drive/MyDrive/True.csv'\n",
        "fake_path = '/content/drive/MyDrive/Fake.csv'\n",
        "\n",
        "true_df = pd.read_csv(true_path)\n",
        "fake_df = pd.read_csv(fake_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N50sXLI1sd50"
      },
      "outputs": [],
      "source": [
        "display(true_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-xrepQ7sfKU"
      },
      "outputs": [],
      "source": [
        "display(fake_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_3SYIyCs6nR"
      },
      "source": [
        "# Data Visulization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqqhNRZOs89i"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(y=\"subject\", palette=\"coolwarm\", data=true_df).set_title('True News Subject Distribution')\n",
        "plt.show()\n",
        "\n",
        "sns.countplot(y=\"subject\", palette=\"coolwarm\", data=fake_df).set_title('Fake News Subject Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkCYrEnI1h4u"
      },
      "source": [
        "## Real News Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG7pmXke1rv2"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "real_titles = true_df.title\n",
        "real_titles_ls = [text for text in real_titles]\n",
        "# print(alls)\n",
        "real_all_words = ' '.join(real_titles)\n",
        "wordcloud_real = WordCloud(background_color='white',\n",
        "    width= 800, height= 500,\n",
        "    max_font_size = 180,\n",
        "    collocations = False).generate(real_all_words)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(wordcloud_real, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SJusV8A1qG-"
      },
      "source": [
        "## Fake News Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLDEsnjE4oc9"
      },
      "outputs": [],
      "source": [
        "fake_titles = fake_df.title\n",
        "fake_titles_ls = [text for text in fake_titles]\n",
        "# print(alls)\n",
        "fake_all_words = ' '.join(fake_titles)\n",
        "wordcloud_fake = WordCloud(background_color='white',\n",
        "    width= 800, height= 500,\n",
        "    max_font_size = 180,\n",
        "    collocations = False).generate(fake_all_words)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(wordcloud_fake, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo5jvavcHOMo"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcPoTiA3tdP6"
      },
      "source": [
        "## Data Combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl80aA_utiKz"
      },
      "outputs": [],
      "source": [
        "# Add Labels to both df\n",
        "true_df['true'] = 1\n",
        "fake_df['true'] = 0\n",
        "\n",
        "# Concat\n",
        "df = pd.concat([true_df, fake_df])\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NehMnA6k0XYN"
      },
      "source": [
        "## Inspect Lengths of News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeFTp8QUyuDY"
      },
      "outputs": [],
      "source": [
        "titles = [text for text in df.title]\n",
        "\n",
        "max_len = 0\n",
        "titles_len = []\n",
        "for title in titles:\n",
        "    titles_len.append(len(title.split()))\n",
        "    max_len = max(len(title.split()), max_len)\n",
        "\n",
        "print('Number of titles:', len(titles))\n",
        "print('Max length of the titles:', max_len)\n",
        "print('Mean length of the titles:', np.mean(titles_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vml-vbR16I88"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "g = sns.countplot(x=titles_len)\n",
        "g.set_xticklabels(g.get_xticklabels(), rotation=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry2QPtxM2kZT"
      },
      "outputs": [],
      "source": [
        "texts = [text for text in df.text]\n",
        "\n",
        "max_len = 0\n",
        "texts_len = []\n",
        "for text in texts:\n",
        "    texts_len.append(len(text.split()))\n",
        "    max_len = max(len(text.split()), max_len)\n",
        "\n",
        "# g = sns.countplot(x=texts_len)\n",
        "print('Mean length of the texts:', np.mean(texts_len))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQjq3-vKGI5"
      },
      "source": [
        "## Purify & Shffle the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_UqiFnsKE-l"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Purify\n",
        "df = df.iloc[:,[0, -1]]\n",
        "\n",
        "# Shuffle\n",
        "df = shuffle(df).reset_index(drop=True)\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGMQocRLjOtt"
      },
      "source": [
        "## Split Data into Training, Validation, Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqfHJBEVjTkH"
      },
      "outputs": [],
      "source": [
        "train_val_df = df.sample(frac = 0.8)\n",
        "test_df = df.drop(train_val_df.index)\n",
        "\n",
        "train_df = train_val_df.sample(frac = 0.8)\n",
        "val_df = train_val_df.drop(train_df.index)\n",
        "\n",
        "# Reset Index\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print('trainset size:', train_df.shape)\n",
        "print('valset size:', val_df.shape)\n",
        "print('testset size:', test_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZCwul6ZhM1F"
      },
      "source": [
        "## Dataframe to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dNUxG_QjmbM"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv('train.tsv', sep='\\t', index=False)\n",
        "val_df.to_csv('val.tsv', sep='\\t', index=False)\n",
        "test_df.to_csv('test.tsv', sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8aYhxfUErP-"
      },
      "source": [
        "## Concatenate all dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbsft39OD1Ny"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([train_df, val_df, test_df])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH0MVDwqU_Bh"
      },
      "source": [
        "## Performing Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPLu2ZvfU6VA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# Downloading Stopwords\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru9_6M-XU3-8"
      },
      "outputs": [],
      "source": [
        "# Obtaining Additional Stopwords From nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuX-0s3mU1rm"
      },
      "outputs": [],
      "source": [
        "# Removing Stopwords And Remove Words With 2 Or Less Characters\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
        "            result.append(token)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "dpMgGBB7rCij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.25.2\n",
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "9sc47eFIpuDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "kStorNn4pvMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFU3Q3KEUzeA"
      },
      "outputs": [],
      "source": [
        "# Applying The Function To The Dataframe\n",
        "df['clean'] = df['title'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7eeTrAjFc8Y"
      },
      "source": [
        "## Obtaining The Total Words Present In The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzI_0g3RFWmk"
      },
      "outputs": [],
      "source": [
        "list_of_words = []\n",
        "for i in df.clean:\n",
        "    for j in i:\n",
        "        list_of_words.append(j)\n",
        "\n",
        "total_words = len(list(set(list_of_words)))\n",
        "total_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aPGNCBzEt4n"
      },
      "source": [
        "## Preparing The Data By Performing Tokenization And Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjDq60T_E1ME"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7S1V_-pIe-I"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "\n",
        "# Creating A Tokenizer To Tokenize The Words And Create Sequences Of Tokenized Words\n",
        "tokenizer = Tokenizer(num_words = total_words)\n",
        "tokenizer.fit_on_texts(train_df['title'])\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_df['title'])\n",
        "val_sequences = tokenizer.texts_to_sequences(val_df['title'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_df['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nu_NxaALTMM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Adding Padding\n",
        "padded_train = pad_sequences(train_sequences,maxlen = 42, padding = 'post', truncating = 'post')\n",
        "padded_val = pad_sequences(val_sequences,maxlen = 42, padding = 'post', truncating = 'post')\n",
        "padded_test = pad_sequences(test_sequences,maxlen = 42, padding = 'post', truncating = 'post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB98lcaHD2YT"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB9GR1q_MGWZ"
      },
      "source": [
        "## Building And Training LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY35--p5MIHX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout, BatchNormalization\n",
        "\n",
        "## Creating model Using LSTM\n",
        "embedding_vector_features=40\n",
        "model=Sequential()\n",
        "model.add(Embedding(total_words,embedding_vector_features,input_length=20))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVrvDwonMROc"
      },
      "outputs": [],
      "source": [
        "y_train = np.asarray(train_df['true'])\n",
        "y_val = np.asarray(val_df['true'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(padded_train, y_train, batch_size = 64, validation_data=(padded_val, y_val), epochs = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0pGfIeLMrGA"
      },
      "source": [
        "## Assessing Trained Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekYcyjswMsJo"
      },
      "outputs": [],
      "source": [
        "# Making prediction\n",
        "prediction = np.argmax(model.predict(padded_test), axis=-1)\n",
        "\n",
        "# Getting The Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_test = np.asarray(test_df['true'])\n",
        "\n",
        "accuracy = accuracy_score(list(y_test), prediction)\n",
        "\n",
        "print(\"LSTM Model Accuracy : \", accuracy)\n",
        "\n",
        "\n",
        "# Getting The Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(list(y_test), prediction)\n",
        "plt.figure(figsize = (6, 6))\n",
        "sns.heatmap(cm, annot = True)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i8UP9N4Jaw1"
      },
      "outputs": [],
      "source": [
        "# SVM Model Implementation\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(train_df['title'])\n",
        "X_val_tfidf = vectorizer.transform(val_df['title'])\n",
        "X_test_tfidf = vectorizer.transform(test_df['title'])\n",
        "\n",
        "# SVM Model\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_tfidf, train_df['true'])\n",
        "\n",
        "# Predictions\n",
        "svm_predictions = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(test_df['true'], svm_predictions)\n",
        "print(\"SVM Model Accuracy: \", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_df['true'], svm_predictions)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True)\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(test_df['true'], svm_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTyZZlXxJgkp"
      },
      "outputs": [],
      "source": [
        "# Naive Bayes Model Implementation\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Naive Bayes Model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, train_df['true'])\n",
        "\n",
        "# Predictions\n",
        "nb_predictions = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(test_df['true'], nb_predictions)\n",
        "print(\"Naive Bayes Model Accuracy: \", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_df['true'], nb_predictions)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True)\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(test_df['true'], nb_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS1Jymp9Ji3r"
      },
      "outputs": [],
      "source": [
        "# Random Forest Model Implementation\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_tfidf, train_df['true'])\n",
        "\n",
        "# Predictions\n",
        "rf_predictions = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(test_df['true'], rf_predictions)\n",
        "print(\"Random Forest Model Accuracy: \", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_df['true'], rf_predictions)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True)\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(test_df['true'], rf_predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO9HZSDcI8Rl"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DdtUeEUJCNk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QHv7lzHJ2gd"
      },
      "outputs": [],
      "source": [
        "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJaw0VtDGPzi"
      },
      "source": [
        "## Load Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F61NjFmGcJDd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, mode, tokenizer):\n",
        "        assert mode in ['train', 'val', 'test']\n",
        "        self.mode = mode\n",
        "        self.df = pd.read_csv(mode + '.tsv', sep='\\t').fillna(\"\")\n",
        "        self.len = len(self.df)\n",
        "        self.tokenizer = tokenizer  # BERT tokenizer\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == 'test':\n",
        "            statement, label = self.df.iloc[idx, :].values\n",
        "            label_tensor = torch.tensor(label)\n",
        "        else:\n",
        "            statement, label = self.df.iloc[idx, :].values\n",
        "            label_tensor = torch.tensor(label)\n",
        "\n",
        "\n",
        "        word_pieces = ['[CLS]']\n",
        "        statement = self.tokenizer.tokenize(statement)\n",
        "        word_pieces += statement + ['[SEP]']\n",
        "        len_st = len(word_pieces)\n",
        "\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "        tokens_tensor = torch.tensor(ids)\n",
        "\n",
        "\n",
        "        segments_tensor = torch.tensor([0] * len_st, dtype=torch.long)\n",
        "\n",
        "        return (tokens_tensor, segments_tensor, label_tensor)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "# Initialize Datasets for Transformation\n",
        "trainset = FakeNewsDataset('train', tokenizer=tokenizer)\n",
        "valset = FakeNewsDataset('val', tokenizer=tokenizer)\n",
        "testset = FakeNewsDataset('test', tokenizer=tokenizer)\n",
        "\n",
        "print('trainset size:' ,trainset.__len__())\n",
        "print('valset size:',valset.__len__())\n",
        "print('testset size: ',testset.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L91EOQVkGbL7"
      },
      "source": [
        "## Sampling and Observing Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1idsU6_zk9h"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "\n",
        "statement, label = trainset.df.iloc[sample_idx].values\n",
        "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
        "combined_text = \" \".join(tokens)\n",
        "print(f\"\"\"\n",
        "original_statement:\n",
        "{statement}\n",
        "\n",
        "tokens:\n",
        "{tokens}\n",
        "\n",
        "label: {label}\n",
        "\n",
        "--------------------\n",
        "\n",
        "tokens_tensor:\n",
        "{tokens_tensor}\n",
        "\n",
        "segments_tensor:\n",
        "{segments_tensor}\n",
        "\n",
        "label_tensor:\n",
        "{label_tensor}\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucRDSbGIGs9W"
      },
      "source": [
        "## Reforming the Dataset to Fit the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17EOIZBt4FAp"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def create_mini_batch(samples):\n",
        "    tokens_tensors = [s[0] for s in samples]\n",
        "    segments_tensors = [s[1] for s in samples]\n",
        "\n",
        "    if samples[0][2] is not None:\n",
        "        label_ids = torch.stack([s[2] for s in samples])\n",
        "    else:\n",
        "        label_ids = None\n",
        "\n",
        "    # Zero Padding\n",
        "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
        "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
        "\n",
        "\n",
        "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
        "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
        "\n",
        "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
        "valloader = DataLoader(valset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVSHXtGi5UJR"
      },
      "outputs": [],
      "source": [
        "data = next(iter(trainloader))\n",
        "\n",
        "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
        "\n",
        "print(f\"\"\"\n",
        "tokens_tensors.shape   = {tokens_tensors.shape}\n",
        "{tokens_tensors}\n",
        "------------------------\n",
        "segments_tensors.shape = {segments_tensors.shape}\n",
        "{segments_tensors}\n",
        "------------------------\n",
        "masks_tensors.shape    = {masks_tensors.shape}\n",
        "{masks_tensors}\n",
        "------------------------\n",
        "label_ids.shape        = {label_ids.shape}\n",
        "{label_ids}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPEO_bTwFuYK"
      },
      "source": [
        "## Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW_20Qy65gv3"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
        "NUM_LABELS = 2\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"\"\"\n",
        "name             module\n",
        "-----------------------\"\"\")\n",
        "for name, module in model.named_children():\n",
        "    if name == \"bert\":\n",
        "        for n, _ in module.named_children():\n",
        "            print(f\"{name}:{n}\")\n",
        "    else:\n",
        "        print(\"{:16} {}\".format(name, module))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snNWm_UR6E7M"
      },
      "outputs": [],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ8UbmbBFkwX"
      },
      "source": [
        "# Fine-Tuning of BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-5eIVJk69IW"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "model = model.to(device)\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "\n",
        "    loop = tqdm(trainloader)\n",
        "    for batch_idx, data in enumerate(loop):\n",
        "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=tokens_tensors,\n",
        "                        token_type_ids=segments_tensors,\n",
        "                        attention_mask=masks_tensors,\n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        logits = outputs[1]\n",
        "        _, pred = torch.max(logits.data, 1)\n",
        "        train_acc = accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
        "\n",
        "        # batch loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # if batch_idx == len(trainloader)-1:\n",
        "        #     _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
        "\n",
        "        loop.set_description(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
        "        loop.set_postfix(acc = train_acc, loss = train_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGCXb2ZmyzzO"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0A4tXryy1p0"
      },
      "outputs": [],
      "source": [
        "torch.save(model, './best_model.pth')\n",
        "print('Model saved!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzgrW9tVyjh4"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8EiVdPKyi_R"
      },
      "outputs": [],
      "source": [
        "# model = torch.load('./best_model.pth')\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyWQAifDIGd_"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RFbGrgVDID-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "true=[]\n",
        "predictions=[]\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for data in testloader:\n",
        "        if next(model.parameters()).is_cuda:\n",
        "            data = [t.to(device) for t in data if t is not None]\n",
        "\n",
        "        tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
        "        test_outputs = model(input_ids=tokens_tensors,\n",
        "                    token_type_ids=segments_tensors,\n",
        "                    attention_mask=masks_tensors)\n",
        "\n",
        "        logits = test_outputs[0]\n",
        "        _, pred = torch.max(logits.data, 1)\n",
        "\n",
        "        labels = data[3]\n",
        "        true.extend(labels.cpu().tolist())\n",
        "        predictions.extend(pred.cpu().tolist())\n",
        "\n",
        "\n",
        "cm = confusion_matrix(true, predictions, labels=[1, 0], normalize='pred')\n",
        "print(cm)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
        "disp.plot()\n",
        "\n",
        "print('Acc: ', accuracy_score(predictions,true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTEIwWN8IVWS"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\"pred_label\": predictions})\n",
        "\n",
        "df_pred = pd.concat([testset.df.loc[:, ['title']],\n",
        "                          testset.df.loc[:, ['true']],\n",
        "                          df.loc[:, 'pred_label']], axis=1)\n",
        "# df_pred.to_csv('bert_1_prec_training_samples.csv', index=False)\n",
        "df_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cemLGn82Ip_r"
      },
      "outputs": [],
      "source": [
        "print(classification_report(df_pred.true, df_pred.pred_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RLlblX-GjNa"
      },
      "source": [
        "## Insight on Wrong Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJEZRa5gGCV1"
      },
      "outputs": [],
      "source": [
        "wrong_df = df_pred[df_pred.true != df_pred.pred_label]\n",
        "sns.countplot(y=\"true\", palette=\"coolwarm\", data=wrong_df).set_title('Wrong Classification Result Real/Fake Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alPx6RBOFhTY"
      },
      "outputs": [],
      "source": [
        "wrong_titles = df_pred[df_pred.true != df_pred.pred_label].title.values\n",
        "wrong_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXuTU2I7Ldic"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "true = []\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for data in testloader:\n",
        "        if next(model.parameters()).is_cuda:\n",
        "            data = [t.to(device) for t in data if t is not None]\n",
        "\n",
        "        tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
        "        test_outputs = model(input_ids=tokens_tensors,\n",
        "                             token_type_ids=segments_tensors,\n",
        "                             attention_mask=masks_tensors)\n",
        "\n",
        "        logits = test_outputs[0]\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)  # Get predicted class labels\n",
        "        # Ensure true labels are added as individual elements, not arrays\n",
        "        true.extend(data[3].cpu().numpy().tolist())  # True labels\n",
        "        predictions.extend(preds.cpu().numpy().tolist())  # Predicted labels\n",
        "\n",
        "# Confusion Matrix and Accuracy\n",
        "cm = confusion_matrix(true, predictions)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(true, predictions)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ykl-ARU6Lqjc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertForSequenceClassification\n",
        "from torch.nn.utils.rnn import pad_sequence  # Import pad_sequence\n",
        "\n",
        "# Load the saved model\n",
        "# model = torch.load('/content/best_model.pth')\n",
        "# model = model.to(device)\n",
        "# Load the saved model with `safe_globals` context manager\n",
        "with torch.serialization.safe_globals([BertForSequenceClassification]):  # Allowlist global\n",
        "    model = torch.load('/content/best_model.pth', weights_only=False)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample text input for prediction\n",
        "sample_texts = [\"This is a sample news article about politics.\", \"Fake news is spreading on social media.\"]\n",
        "\n",
        "# Tokenize the input\n",
        "def preprocess_text(texts):\n",
        "    word_pieces = ['[CLS]']\n",
        "    tokenized_texts = [tokenizer.tokenize(text) for text in texts]\n",
        "    word_pieces += [item for sublist in tokenized_texts for item in sublist] + ['[SEP]']\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "    return input_ids\n",
        "\n",
        "input_ids = [preprocess_text([text]) for text in sample_texts]\n",
        "\n",
        "# Pad sequences to the same length\n",
        "input_ids = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0).to(device) # Pad sequences\n",
        "\n",
        "# Prepare DataLoader for prediction\n",
        "class PredictionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids):\n",
        "        self.input_ids = input_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx]\n",
        "\n",
        "predict_dataset = PredictionDataset(input_ids)\n",
        "predict_loader = DataLoader(predict_dataset, batch_size=1)\n",
        "\n",
        "# Make predictions\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for data in predict_loader:\n",
        "        tokens_tensor = data.to(device)\n",
        "        outputs = model(input_ids=tokens_tensor)\n",
        "        logits = outputs[0]\n",
        "        _, predicted_class = torch.max(logits, dim=1)\n",
        "        predictions.append(predicted_class.cpu().item())\n",
        "\n",
        "# Display the predictions\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {'Fake' if predictions[i] == 0 else 'True'}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7O8ipAIo8UF"
      },
      "source": [
        "**Building Streamlit app**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2Rv20XHo65n"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7brC2VLpAr7"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import streamlit as st\n",
        "\n",
        "# Set the device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the entire saved model (no need for state_dict here)\n",
        "model = torch.load('/content/best_model.pth', weights_only=False)\n",
        "model = model.to(device)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Streamlit App UI\n",
        "st.title('Fake News Detection')\n",
        "st.write(\"Enter a news article or text below:\")\n",
        "\n",
        "# Text input box\n",
        "input_text = st.text_area(\"Input Text\", \"Enter Text.\")\n",
        "\n",
        "if st.button('Predict'):\n",
        "    if not input_text.strip():  # Check if input is empty or only whitespace\n",
        "        st.error(\"Input Cannot be empty\")\n",
        "    else:\n",
        "        # Tokenize the input text\n",
        "        inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "        input_ids = inputs['input_ids'].to(device)\n",
        "        attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Make predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        _, predicted_class = torch.max(logits, dim=1)\n",
        "        prediction = 'Fake' if predicted_class.item() == 0 else 'True'\n",
        "\n",
        "    # Display prediction\n",
        "    st.write(f\"Prediction: {prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmGeV5ewtrdx"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwhWQPS9tuol"
      },
      "outputs": [],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsCcahHDt0cr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PawDtEeT2Smv"
      },
      "outputs": [],
      "source": [
        "!pip install flask transformers torch\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}